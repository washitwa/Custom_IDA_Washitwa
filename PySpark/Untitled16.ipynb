{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbd0370e",
   "metadata": {},
   "source": [
    "## Serialization, Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb677b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this code beofre creating any context or session\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d319f10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c9896f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.4 (main, Jul  5 2023, 14:15:25) [GCC 11.2.0]\n"
     ]
    }
   ],
   "source": [
    "#check the Python Version\n",
    "\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fcee30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/25 05:44:16 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/25 05:44:17 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#create the spark context and session\n",
    "\n",
    "conf = SparkConf().setAppName(\"NewApp\").setMaster(\"local\")\n",
    "sc = SparkContext(conf = conf)\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b13dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list\n",
    "\n",
    "test_df = [(\"James\",\"Sales\",\"NY\",90000,34,10000),\n",
    "    (\"Michael\",\"Sales\",\"NY\",86000,56,20000),\n",
    "    (\"Robert\",\"Sales\",\"CA\",81000,30,23000),\n",
    "    (\"Maria\",\"Finance\",\"CA\",90000,24,23000),\n",
    "    (\"Raman\",\"Finance\",\"CA\",99000,40,24000),\n",
    "    (\"Scott\",\"Finance\",\"NY\",83000,36,19000),\n",
    "    (\"Jen\",\"Finance\",\"NY\",79000,53,15000),\n",
    "    (\"Jeff\",\"Marketing\",\"CA\",80000,25,18000),\n",
    "    (\"Kumar\",\"Marketing\",\"NY\",91000,50,21000)\n",
    "  ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b776c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the schema\n",
    "\n",
    "ud_schema = [\"employee_name\",\"department\",\"state\",\"salary\",\"age\",\"bonus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "645814c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe\n",
    "\n",
    "df = spark.createDataFrame(data=test_df,schema = ud_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ecf3ccb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#show the dataframe\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40156b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cache the dataframe\n",
    "\n",
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c3d95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serialization\n",
    "#StorageLevel decides how RDD should be stored. In Apache Spark, StorageLevel decides whether RDD should be stored in the memory or should it be stored over the disk, or both. \n",
    "#It also decides whether to serialize RDD and whether to replicate RDD partitions.\n",
    "\n",
    "\n",
    "test = StorageLevel(useDisk=False, useMemory=True, useOffHeap=False,deserialized=False,replication = 1)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21588af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.presist(storageLevel=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212be8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregation\n",
    "\n",
    "df.groupBy(\"department\").agg(sum(\"salary\").alias(\"sum_salary\"), avg(\"salary\").alias(\"avg_salary\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "11992829",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to use SQL queries on Dataframes, create a tempView first\n",
    "df.createOrReplaceTempView(\"dftemp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "be05f822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+-----+------+---+-----+\n",
      "|employee_name|department|state|salary|age|bonus|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "|        James|     Sales|   NY| 90000| 34|10000|\n",
      "|      Michael|     Sales|   NY| 86000| 56|20000|\n",
      "|       Robert|     Sales|   CA| 81000| 30|23000|\n",
      "|        Maria|   Finance|   CA| 90000| 24|23000|\n",
      "|        Raman|   Finance|   CA| 99000| 40|24000|\n",
      "|        Scott|   Finance|   NY| 83000| 36|19000|\n",
      "|          Jen|   Finance|   NY| 79000| 53|15000|\n",
      "|         Jeff| Marketing|   CA| 80000| 25|18000|\n",
      "|        Kumar| Marketing|   NY| 91000| 50|21000|\n",
      "+-------------+----------+-----+------+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.sql(\"select * from dftemp\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1e510f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check which database is being used\n",
    "#select database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fdd929a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the number of partitions\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3ed4bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.format(\"csv\").partitionBy(\"department\").mode(\"overwrite\").save(\"/home/labuser/Downloads/Partitions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8492904",
   "metadata": {},
   "source": [
    "if the number of partitions are changed using repartition, then shuffling occurs which is not always wanted\n",
    "\n",
    "\n",
    "hence, if the number of partitions are to be reduced, then coalesce should be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3ce2f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Row(employee_name='James', department='Sales', state='NY', salary=90000, age=34, bonus=10000)\n",
      "Row(employee_name='Michael', department='Sales', state='NY', salary=86000, age=56, bonus=20000)\n",
      "Row(employee_name='Robert', department='Sales', state='CA', salary=81000, age=30, bonus=23000)\n",
      "Row(employee_name='Maria', department='Finance', state='CA', salary=90000, age=24, bonus=23000)\n",
      "Row(employee_name='Raman', department='Finance', state='CA', salary=99000, age=40, bonus=24000)\n",
      "Row(employee_name='Scott', department='Finance', state='NY', salary=83000, age=36, bonus=19000)\n",
      "Row(employee_name='Jen', department='Finance', state='NY', salary=79000, age=53, bonus=15000)\n",
      "Row(employee_name='Jeff', department='Marketing', state='CA', salary=80000, age=25, bonus=18000)\n",
      "Row(employee_name='Kumar', department='Marketing', state='NY', salary=91000, age=50, bonus=21000)\n"
     ]
    }
   ],
   "source": [
    "#foreach\n",
    "df.foreach(print)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a7ada",
   "metadata": {},
   "source": [
    "## Complex Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a8d0bdcb",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "StructType.__init__() takes from 1 to 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m empSchema \u001b[38;5;241m=\u001b[39m StructType([StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDepartment\u001b[39m\u001b[38;5;124m\"\u001b[39m,IntegerType(),\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      2\u001b[0m                         StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m,StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      3\u001b[0m                         StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAge\u001b[39m\u001b[38;5;124m\"\u001b[39m,StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      4\u001b[0m                         StructField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAddress\u001b[39m\u001b[38;5;124m\"\u001b[39m, StructType([\n\u001b[0;32m----> 5\u001b[0m                             StructType(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCity\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m      6\u001b[0m                             StructType(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mState\u001b[39m\u001b[38;5;124m\"\u001b[39m, StringType(), \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m             ]),\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m ])\n",
      "\u001b[0;31mTypeError\u001b[0m: StructType.__init__() takes from 1 to 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "empSchema = StructType([StructField(\"Department\",IntegerType(),True),\n",
    "                        StructField(\"Name\",StringType(), True),\n",
    "                        StructField(\"Age\",StringType(), True),\n",
    "                        StructField(\"Address\", StructType([\n",
    "                            StructType(\"City\", StringType(), True),\n",
    "                            StructType(\"State\", StringType(), True)\n",
    "            ]),True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c666cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"address\", StructType([\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"state\", StringType(), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "\n",
    "df = spark.read.schema(schema).json(spark.sparkContext.parallelize(json_data))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
