{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d34b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bdfcbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf = SparkConf().setAppName(\"NewApp\").setMaster(\"local\")\n",
    "# sc= SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00648fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/22 08:24:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/09/22 08:24:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#spark = SparkSession.builder.getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"NewApp\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ee5208",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c56ebca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "purdf = spark.read.format(\"csv\").option(\"inferSchema\",True).option(\"header\",True).load(\"/home/labuser/Downloads/purchases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ebd9f8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+\n",
      "|  Name|apples|oranges|\n",
      "+------+------+-------+\n",
      "|  June|     3|      0|\n",
      "|Robert|     2|      3|\n",
      "|  Lily|     0|      7|\n",
      "| David|     1|      2|\n",
      "+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "502619a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+\n",
      "|  Name|apples|oranges|\n",
      "+------+------+-------+\n",
      "|  June|     3|      0|\n",
      "|Robert|     2|      3|\n",
      "|  Lily|     0|      7|\n",
      "| David|     1|      2|\n",
      "+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purdf = purdf.withColumnRenamed(\"_c0\", \"Name\")\n",
    "purdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94daa41f",
   "metadata": {},
   "source": [
    "# Spark Partitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee88f60e",
   "metadata": {},
   "source": [
    "## Write Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11cc943f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the number of partitions fpr purdf dataframe\n",
    "purdf.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fc73369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting the number of partitions to 6 for the dataframe purdf\n",
    "purdf = purdf.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1423e009",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 6:=======================================>                   (4 + 2) / 6]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "purdf.write.format(\"csv\").mode(\"overwrite\").save(\"/home/labuser/Downloads/writeoperation/purch.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a056375b",
   "metadata": {},
   "source": [
    "As there are only 4 records in the dataframe shown above, a higher number of partitions will still result in a total of 4 partitions as there will be one record in each partition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ac89f3",
   "metadata": {},
   "source": [
    "## Spark UI\n",
    "\n",
    "used to get a user friendly detail of the entire backend process when the Spark Engine runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbb0047b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to run the link directly from the python code\n",
    " \n",
    "import webbrowser\n",
    "\n",
    "url = sc.uiWebUrl\n",
    "\n",
    "webbrowser.open_new_tab(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d391b5",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c6e324",
   "metadata": {},
   "source": [
    "SQL can be used with Dataframes for all kind of transformations (select, filter, groupBy etc.)\n",
    "DSL can also be used with DataFrames for all kinds of transformations (select, filter, groupBy etc.)\n",
    "For RDDs, only lambda operations are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f1c7104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+\n",
      "|  Name|apples|oranges|\n",
      "+------+------+-------+\n",
      "|Robert|     2|      3|\n",
      "| David|     1|      2|\n",
      "|  June|     3|      0|\n",
      "|  Lily|     0|      7|\n",
      "+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select operation\n",
    "purdf.createOrReplaceTempView(\"pur\")\n",
    "\n",
    "df1 = spark.sql(\"select * from pur\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33daa468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+\n",
      "|  Name|apples|oranges|\n",
      "+------+------+-------+\n",
      "| David|     1|      2|\n",
      "|  June|     3|      0|\n",
      "|Robert|     2|      3|\n",
      "+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter operation\n",
    "df1 = spark.sql(\"select * from pur where apples>0\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b3892de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eefb06d",
   "metadata": {},
   "source": [
    "## Spark DSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a74f2207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+\n",
      "|  Name|apples|oranges|\n",
      "+------+------+-------+\n",
      "|Robert|     2|      3|\n",
      "| David|     1|      2|\n",
      "|  June|     3|      0|\n",
      "|  Lily|     0|      7|\n",
      "+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select operation\n",
    "purdf.select('*').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7cb8920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+\n",
      "|  Name|apples|oranges|\n",
      "+------+------+-------+\n",
      "| David|     1|      2|\n",
      "|  June|     3|      0|\n",
      "|Robert|     2|      3|\n",
      "+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#filter operation\n",
    "purdf.filter(col(\"apples\")>0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4873a0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select except few\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7114bbea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+\n",
      "|  Name|apples|oranges|\n",
      "+------+------+-------+\n",
      "|  June|     3|      0|\n",
      "|Robert|     2|      3|\n",
      "| David|     1|      2|\n",
      "|  Lily|     0|      7|\n",
      "+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#sort operation\n",
    "purdf.sort(col(\"apples\").desc()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b0dc8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "|apples|  Name|\n",
      "+------+------+\n",
      "|     2|Robert|\n",
      "|     0|  Lily|\n",
      "|     3|  June|\n",
      "|     1| David|\n",
      "+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select multiple\n",
    "purdf.select(col(\"apples\"), col(\"Name\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e98db10",
   "metadata": {},
   "source": [
    "## When and Otherwise in PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f8823b",
   "metadata": {},
   "source": [
    "There are two ways to provide conditional statements and get the output\n",
    "\n",
    "-> First one is using Expressions: utilises the SQL code to achieve the ouput\n",
    "\n",
    "-> Second one is using DSL when and otherwise concept.\n",
    "\n",
    "-> DSL is more efficient than SQL and therefore is instead used.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e82600c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+--------------+\n",
      "|  Name|apples|oranges|           New|\n",
      "+------+------+-------+--------------+\n",
      "|Robert|     2|      3|  Big Purchase|\n",
      "| David|     1|      2|Small Purchase|\n",
      "|  June|     3|      0|Small Purchase|\n",
      "|  Lily|     0|      7|Small Purchase|\n",
      "+------+------+-------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purdf.withColumn(\"New\", expr(\"CASE WHEN apples>=2 and oranges>=2 THEN 'Big Purchase' ELSE 'Small Purchase' END\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdf89703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|apples|      Purchase|\n",
      "+------+--------------+\n",
      "|     1|Small Purchase|\n",
      "|     3|  Big Purchase|\n",
      "|     0|Small Purchase|\n",
      "|     2|  Big Purchase|\n",
      "+------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purdf.select(\"apples\", when(col(\"apples\")>=2, \"Big Purchase\").otherwise(\"Small Purchase\").alias(\"Purchase\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209a93dd",
   "metadata": {},
   "source": [
    "## User-defined Functions in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "de192e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatShell(purCol):\n",
    "    return purCol + \"_Shell\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d592cc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test_Shell'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatShell(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e483919d",
   "metadata": {},
   "outputs": [],
   "source": [
    "udf_shell = udf(concatShell, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f21e790c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+-------+\n",
      "|  Name|apples|oranges|\n",
      "+------+------+-------+\n",
      "|Robert|     2|      3|\n",
      "| David|     1|      2|\n",
      "|  June|     3|      0|\n",
      "|  Lily|     0|      7|\n",
      "+------+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "purdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "10648b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/22 08:38:44 ERROR Executor: Exception in task 0.0 in stage 40.0 (TID 75)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 683, in main\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Python in worker has different version 3.8 than that in driver 3.11, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/09/22 08:38:44 WARN TaskSetManager: Lost task 0.0 in stage 40.0 (TID 75) (ip-172-31-6-221.ap-south-1.compute.internal executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 683, in main\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Python in worker has different version 3.8 than that in driver 3.11, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:561)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.PythonUDFRunner$$anon$2.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:514)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "23/09/22 08:38:44 ERROR TaskSetManager: Task 0 in stage 40.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 683, in main\n    raise RuntimeError(\nRuntimeError: Python in worker has different version 3.8 than that in driver 3.11, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test \u001b[38;5;241m=\u001b[39m purdf\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnew_col\u001b[39m\u001b[38;5;124m\"\u001b[39m, udf_shell(col(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mName\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m----> 2\u001b[0m test\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/sql/dataframe.py:899\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    894\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    895\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m    896\u001b[0m     )\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[0;32m--> 899\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical))\n\u001b[1;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    901\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    171\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 683, in main\n    raise RuntimeError(\nRuntimeError: Python in worker has different version 3.8 than that in driver 3.11, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n"
     ]
    }
   ],
   "source": [
    "test = purdf.withColumn(\"new_col\", udf_shell(col(\"Name\")))\n",
    "test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4890ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "purdf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48492ba0",
   "metadata": {},
   "source": [
    "## Caching in PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2fb5df36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/22 08:30:05 WARN CacheManager: Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, apples: int, oranges: int]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purdf.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f54bb6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
